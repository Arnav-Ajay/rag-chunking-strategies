question_id,question_intent,question_text,chunks_fixed,chunks_structural,chunks_semantic,gold_chunkids_fixed,gold_chunkids_structural,gold_chunkids_semantic
1,definition,What architectural components are removed in the Transformer model?,Atomic,Atomic,Atomic,1,0,0
2,mechanism,What aspect of recurrent neural networks prevents parallelization during training?,Atomic,Atomic,X,6,4,
3,capability,What capability do attention mechanisms provide for modeling dependencies in sequences?,Atomic,Atomic,Atomic,7,5,4
5,comparative,How does the number of operations required to relate distant positions differ between convolutional models and the Transformer?,Atomic,Atomic,Atomic,9,6,5
13,capability,What advantage does multi-head attention provide?,Atomic,Atomic,Atomic,21,13,12
19,mechanism,What technique is used to mitigate data sparsity in n-gram models?,Atomic,Atomic,Atomic,68,39,38
21,principle,What property of embedding vectors enables semantic similarity computation?,Atomic,Atomic,Atomic,71,40,40
22,comparative,What distinguishes pre-trained language models from early neural language models?,Atomic,Structural,Structural,73,42,41
23,process,What are the two stages of the pre-training and fine-tuning paradigm?,Atomic,Atomic,Structural,74,43,42
24,definition,How are large language models defined in terms of scale and architecture?,Atomic,Structural,Compositional,75,43,"42,43"
25,capability,What emergent abilities are observed in large language models?,Atomic,Structural,Compositional,79,46,"45,46"
35,definition,What types of memory are combined in retrieval-augmented generation models?,Atomic,Atomic,Atomic,515,285,274
37,comparative,How do the two RAG formulations differ in how retrieved passages are used?,Atomic,Structural,Structural,516,285,274
40,mechanism,What search method is used to retrieve top-K documents in RAG?,Atomic,Atomic,Atomic,525,289,277
42,mechanism,What approximation is used to marginalize over retrieved documents during generation?,Atomic,Atomic,Atomic,526,290,278
