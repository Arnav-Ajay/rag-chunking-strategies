question_id,question_intent,question_text,chunks_fixed,chunks_structural,chunks_semantic,Chunkids_fixed,Chunkids_structural,Chunkids_semantic
1,definition,What architectural components are removed in the Transformer model?,Atomic,Atomic,Atomic,1,0,0
2,mechanism,What aspect of recurrent neural networks prevents parallelization during training?,Atomic,Atomic,Atomic,6,4,4
3,capability,What capability do attention mechanisms provide for modeling dependencies in sequences?,Atomic,Atomic,Atomic,7,5,4
4,definition,What mechanism does the Transformer rely on instead of recurrence?,,,,,,
5,comparative,How does the number of operations required to relate distant positions differ between convolutional models and the Transformer?,Atomic,Atomic,Atomic,9,6,5
6,definition,How is self-attention defined?,,,,,,
7,definition,What distinguishes the Transformer from earlier transduction models?,,,,,,
8,definition,What overall encoder–decoder architecture does the Transformer use?,,,,,,
9,definition,What two sub-layers make up each encoder layer?,,,,,,
10,definition,What additional sub-layer is added to each decoder layer?,,,,,,
11,mechanism,Why is masking applied in the decoder self-attention layer?,,,,,,
12,mechanism,What operation is applied to scaled dot products before weighting the values?,,,,,,
13,capability,What advantage does multi-head attention provide?,Atomic,Atomic,Atomic,21,13,12
14,mechanism,How are illegal attention connections prevented in the decoder?,,,,,,
15,mechanism,What is the functional form of the position-wise feed-forward network?,,,,,,
16,mechanism,What mathematical functions are used to construct positional encodings?,,,,,,
17,definition,How do statistical language models compute the probability of text?,,,,,,
18,definition,What context length does an n-gram model condition on?,,,,,,
19,mechanism,What technique is used to mitigate data sparsity in n-gram models?,Atomic,Atomic,Atomic,68,39,38
20,comparative,How do neural language models address data sparsity differently from n-gram models?,,,,,,
21,principle,What property of embedding vectors enables semantic similarity computation?,Atomic,Atomic,Atomic,71,40,40
22,comparative,What distinguishes pre-trained language models from early neural language models?,Atomic,Atomic,Atomic,73,42,41
23,process,What are the two stages of the pre-training and fine-tuning paradigm?,Atomic,Atomic,Atomic,74,43,
24,definition,How are large language models defined in terms of scale and architecture?,Atomic,Atomic,Compositional,75,43,"42,43"
25,capability,What emergent abilities are observed in large language models?,Atomic,Atomic,Compositional,79,46,"45,46"
26,capability,What capability does instruction tuning enable?,,,,,,
27,historical,Which neural architectures were commonly used before Transformers?,,,,,,
28,comparative,What advantage do Transformers have over recurrent neural networks?,,,,,,
29,definition,What components make up the BERT architecture?,,,,,,
30,process,What pre-training objectives are used in BERT?,,,,,,
31,mechanism,How does replaced token detection differ from masked language modeling?,,,,,,
32,process,What training approaches are used in XLM?,,,,,,
33,comparative,What architectural modifications distinguish LLaMA from GPT-3?,,,,,,
34,principle,What scaling rule does the Chinchilla study identify for compute-optimal training?,,,,,,
35,definition,What types of memory are combined in retrieval-augmented generation models?,Atomic,Atomic,Atomic,515,285,274
36,definition,What serves as the non-parametric memory in RAG models?,,,,,,
37,comparative,How do the two RAG formulations differ in how retrieved passages are used?,Atomic,Atomic,Atomic,516,285,274
38,principle,What limitations of purely parametric language models are identified?,,,,,,
39,definition,Which models combine masked language models with a differentiable retriever?,,,,,,
40,mechanism,What search method is used to retrieve top-K documents in RAG?,Atomic,Atomic,Atomic,525,289,277
41,definition,Which sequence-to-sequence model is used as the generator in RAG?,,,,,,
42,mechanism,What approximation is used to marginalize over retrieved documents during generation?,Atomic,Atomic,Atomic,526,290,278
43,comparative,How does RAG approach differ from training non-parametric memory from scratch?,,,,,,
44,historical,Which open-domain QA datasets achieve state-of-the-art results with RAG?,,,,,,
45,definition,What components make up a retrieval-augmented generation model?,,,,,,
46,mechanism,What assumption does the RAG-Sequence model make about retrieved documents?,,,,,,
47,comparative,What capability does RAG-Token introduce compared to RAG-Sequence?,,,,,,
48,process,How can RAG be adapted for sequence classification?,,,,,,
49,mechanism,What similarity operation is used to score query–document pairs in DPR?,,,,,,
50,historical,On which datasets was the DPR retriever originally trained?,,,,,,
51,definition,What model is used as the generator in the RAG framework?,,,,,,
52,process,"Which components are fine-tuned during training, and which are kept fixed?",,,,,,
53,comparative,What distinguishes Fast Decoding from Thorough Decoding in RAG?,,,,,,
54,mechanism,How can RAG models update world knowledge without retraining?,,,,,,
